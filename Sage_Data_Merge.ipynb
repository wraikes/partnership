{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sage Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, pandas as pd, numpy as np, re\n",
    "from functools import reduce\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "#NOTE TO SELF: redo the relative paths.\n",
    "sage = '/home/wraikes/Dropbox/partnership/DMTBilly data - Copy/Sage Data'\n",
    "os.chdir(sage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_dupes(df):\n",
    "    #'Must confirm removal of these dupes, especially ThpMV2Achc\n",
    "    test_users = ['ULoF3MM1nN', 'gSWn9N', 'D5bzYrfd8E', 'ThpMV2Achc']\n",
    "    return df[~df.externalId.isin(test_users)]\n",
    "\n",
    "def dupe_check(df):\n",
    "    return len(df.externalId) == len(df.externalId.unique()) and len(df.externalId) > 0\n",
    "\n",
    "def new_cols(df, append):\n",
    "    # Remove: 'metadata.json.' and 'data.json.'.\n",
    "    col_re = re.compile('metadata.json.|data.json.')\n",
    "    df.columns = list(map(lambda x: re.sub(col_re, '', x), df.columns))\n",
    "    \n",
    "    # Append df identifier to columns.\n",
    "    df.columns = list(map(lambda x: x + append, df.columns))\n",
    "    df = df.rename(columns = {'externalId' + append: 'externalId'})\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_new_df(df, att, var_1, var_2=None):\n",
    "    cols = df.columns\n",
    "    \n",
    "    new_df = pd.DataFrame(columns = cols)\n",
    "        \n",
    "    for ix, row in df.iterrows():\n",
    "        if var_2:\n",
    "            if row[var_1] == att[1] and row[var_2] == att[2]:\n",
    "                new_df = new_df.append(row, ignore_index=True)\n",
    "        else:\n",
    "            if row[var_1] == att[1]:\n",
    "                new_df = new_df.append(row, ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def df_merge(df, attributes, var_1, var_2=None):\n",
    "    dfs = []\n",
    "    \n",
    "    for att in attributes:\n",
    "        new_df = create_new_df(df, att, var_1, var_2)\n",
    "        new_df = new_cols(new_df, att[0])\n",
    "        dfs.append(new_df)\n",
    "    \n",
    "    df_merge = reduce(lambda left, right: pd.merge(left, \n",
    "                                                   right, \n",
    "                                                   how = 'outer',\n",
    "                                                   on='externalId'),\n",
    "                      dfs)\n",
    "    \n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process files that do not need cleaning / restructuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_users = ['ULoF3MM1nN', 'gSWn9N', 'D5bzYrfd8E', 'ThpMV2Achc']\n",
    "files_to_exclude = ['digital-marshmallow-status_8.8.17.csv',\n",
    "                    'digital-marshmallow-appVersion_8.8.17.csv']\n",
    "\n",
    "files_as_is = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file not in files_to_exclude:\n",
    "        df = pd.read_csv(file)\n",
    "        df = remove_dupes(df)\n",
    "        if dupe_check(df):\n",
    "            files_as_is.append(file)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_as_is = ['_-_past_year_likert_21', \n",
    "               '_-_behavior_choices_1',\n",
    "               '_-_comments_21',\n",
    "               '_-_demos',\n",
    "               '_-_behavior_lk_21',\n",
    "               '_-_behavior_lk_bl',\n",
    "               '_-_generally_sem_bl',\n",
    "               '_-_generally_sem_21',\n",
    "               '_-_as_a_child',\n",
    "               '_-_past_year_likert_bl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dfs = {}\n",
    "\n",
    "for file, name in zip(files_as_is, names_as_is):\n",
    "    df = pd.read_csv(file)\n",
    "    df = remove_dupes(df)\n",
    "    df = new_cols(df, name)\n",
    "    new_dfs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([dupe_check(df) for name, df in new_dfs.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame: Bart_V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_v4 = pd.read_csv('digital-marshmallow-bart-v4_8.8.17.csv')\n",
    "bart_v4 = remove_dupes(bart_v4)\n",
    "\n",
    "dupe_check(bart_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bart_attributes = [\n",
    "    ['_-_bart_v4_bl_0.25', 'baseline', 'BART0.25'],\n",
    "    ['_-_bart_v4_bl_250', 'baseline', 'BART250.00'],\n",
    "    ['_-_bart_v4_21_0.25', '21-day-assessment', 'BART250.00'],\n",
    "    ['_-_bart_v4_21_250', '21-day-assessment', 'BART250.00']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bart_v4 = df_merge(bart_v4, \n",
    "                   bart_attributes,\n",
    "                   var_1='metadata.json.taskIdentifier',\n",
    "                   var_2='data.json.variable_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dupe_check(bart_v4):\n",
    "    new_dfs['bart_v4'] = bart_v4\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame: Behavior_choices_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_4 = pd.read_csv('digital-marshmallow-behavior_choices_4_bl-v2_8.8.17.csv')\n",
    "behavior_4 = remove_dupes(behavior_4)\n",
    "\n",
    "dupe_check(behavior_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "behavior_4_attributes = [\n",
    "    ['_-_behavior_4_bl', 'baseline'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "behavior_4 = df_merge(behavior_4, \n",
    "                      behavior_4_attributes,\n",
    "                      var_1='metadata.json.taskIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dupe_check(behavior_4):\n",
    "    new_dfs['behave_4'] = behavior_4\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame: Delay Discounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = pd.read_csv('digital-marshmallow-delay_discounting_raw-v6_8.8.17.csv')\n",
    "delay = remove_dupes(delay)\n",
    "\n",
    "dupe_check(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bl = 'baseline'\n",
    "_21 = '21-day-assessment'\n",
    "\n",
    "delay_attributes = [\n",
    "    ['_-_delay_bl_time_6_month', bl, 'dd_time_6_month'],\n",
    "    ['_-_delay_bl_money_6_month', bl, 'dd_money_6_month'],\n",
    "    ['_-_delay_bl_money_1_month', bl, 'dd_money_1_month'],\n",
    "    ['_-_delay_bl_time_1_year', bl, 'dd_time_1_year'],\n",
    "    ['_-_delay_21_time_6_month', _21, 'dd_time_6_month'],\n",
    "    ['_-_delay_21_money_6_month', _21, 'dd_money_6_month'],\n",
    "    ['_-_delay_21_money_1_month', _21, 'dd_money_1_month'],\n",
    "    ['_-_delay_21_time_1_year', _21, 'dd_time_1_year']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delay = df_merge(delay, \n",
    "                 delay_attributes, \n",
    "                 var_1='metadata.json.taskIdentifier',\n",
    "                 var_2='data.json.variableLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dupe_check(delay):\n",
    "    new_dfs['delay'] = delay\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame: Discounting Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = pd.read_csv('digital-marshmallow-discounting_raw-v2_8.8.17.csv')\n",
    "discount = remove_dupes(discount)\n",
    "\n",
    "dupe_check(discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bl = 'baseline'\n",
    "_21 = '21-day-assessment'\n",
    "\n",
    "discount_attributes = [\n",
    "    ['_-_discount_bl_money', bl, 'pd_constant_money'],\n",
    "    ['_-_discount_bl_prob', bl, 'pd_constant_probabiliy'],\n",
    "    ['_-_discount_21_money', _21, 'pd_constant_money'],\n",
    "    ['_-_discount_21_prob', _21, 'pd_constant_probability']\n",
    "]\n",
    "\n",
    "discount = df_merge(discount, \n",
    "                    discount_attributes,\n",
    "                    var_1='metadata.json.taskIdentifier',\n",
    "                    var_2='data.json.variableLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dupe_check(discount):\n",
    "    new_dfs['discount'] = discount\n",
    "    print('Done!')\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame: Evening Notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evening_note = pd.read_csv('digital-marshmallow-evening_notification_time-v2_8.8.17.csv')\n",
    "evening_note = remove_dupes(evening_note)\n",
    "\n",
    "dupe_check(evening_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evening_note_attributes = [\n",
    "    ['_-_evening_note_bl', 'baseline']\n",
    "]\n",
    "\n",
    "evening_note_bl = df_merge(evening_note, \n",
    "                           evening_note_attributes,\n",
    "                           var_1='metadata.json.taskIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dupe_check(evening_note_bl):\n",
    "    new_dfs['evening_note'] = evening_note_bl\n",
    "    print('Done!')\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame: GoNoGo - PLACEHOLDER (Extra Record - ksJM3Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gonogo = pd.read_csv('digital-marshmallow-goNoGo-v2_8.8.17.csv')\n",
    "gonogo = remove_dupes(gonogo)\n",
    "\n",
    "dupe_check(gonogo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bl = 'baseline'\n",
    "_21 = '21-day-assessment'\n",
    "\n",
    "gonogo_attributes = [\n",
    "    ['_-_gonogo_bl_stable', bl, 'go_no_go_stable_stimulus_active_task'],\n",
    "    ['_-_gonogo_21_variable', bl, 'go_no_go_variable_stimulus_active_task'],\n",
    "    ['_-_gonogo_bl_stable', _21, 'go_no_go_stable_stimulus_active_task'],\n",
    "    ['_-_gonogo_21_variable', _21, 'go_no_go_variable_stimulus_active_task']\n",
    "]\n",
    "\n",
    "gonogo = df_merge(gonogo, \n",
    "                  gonogo_attributes,\n",
    "                  var_1='metadata.json.taskIdentifier',\n",
    "                  var_2='data.json.variable_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dupe_check(gonogo):\n",
    "    new_dfs['gonogo'] = gonogo\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gonogo.externalId[gonogo.externalId.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame: Morning Notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_note = pd.read_csv('digital-marshmallow-morning_notification_time-v3_8.8.17.csv')\n",
    "morning_note = remove_dupes(morning_note)\n",
    "\n",
    "dupe_check(morning_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morning_note_attributes = [\n",
    "    ['_-_morning_note_bl', 'baseline']\n",
    "]\n",
    "\n",
    "morning_note = df_merge(morning_note, \n",
    "                        morning_note_attributes,\n",
    "                        var_1='metadata.json.taskIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dupe_check(morning_note):\n",
    "    new_dfs['morning_note'] = morning_note\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame: PAM Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_mult = pd.read_csv('digital-marshmallow-pam_multiple-v2_8.8.17.csv')\n",
    "pam_mult = remove_dupes(pam_mult)\n",
    "\n",
    "dupe_check(pam_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pam_mult_attributes = [\n",
    "    ['_-_pam_mult_bl', 'baseline'],\n",
    "    ['_-_pam_mult_21', '21-day-assessment']\n",
    "]\n",
    "\n",
    "pam_mult = df_merge(pam_mult, \n",
    "                    pam_mult_attributes,\n",
    "                    var_1='metadata.json.taskIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dupe_check(pam_mult):\n",
    "    new_dfs['pam_mult'] = pam_mult\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Merge of All Sage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = reduce(lambda left, right: pd.merge(left, right, how = 'outer',\n",
    "                                               on='externalId'), \n",
    "                  new_dfs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dupe_check(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/wraikes/Programming/Partnership/dmt/merged_data/')\n",
    "final_df.to_csv('FINAL_SAGE.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
